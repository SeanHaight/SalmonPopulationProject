{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4448994",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the relevant Python modules\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9622cc9",
   "metadata": {},
   "source": [
    "# Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "867df0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = 1990\n",
    "end_date = 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fc87a2",
   "metadata": {},
   "source": [
    "# Target Data Reading "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b774063a",
   "metadata": {},
   "source": [
    "## Salmon viability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56aa279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the relevant data\n",
    "Salmon_file = pd.read_excel(\"Data/ZooPlanktonPerryData.xlsx\", 5)\n",
    "\n",
    "#Take years less than or equal to 2017  (year 2018 has only NaN)\n",
    "\n",
    "Salmon = Salmon_file[Salmon_file[\"Ocean Entry Year\"]<=2017]\n",
    "Salmon = Salmon.rename(columns={\"Ocean Entry Year\":\"Year\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bf1d625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salmon types are: ['Cowichan Chinook', 'Harrison Chinook', 'Puntledge Chinook', 'Big Qualicum Coho']\n"
     ]
    }
   ],
   "source": [
    "# List of salmon types \n",
    "year_column = 'Year'\n",
    "salmonTypes_List =  [col for col in Salmon.columns if col != year_column]\n",
    "\n",
    "print(\"Salmon types are:\", salmonTypes_List)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672822f0",
   "metadata": {},
   "source": [
    "# Data Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae73bd5",
   "metadata": {},
   "source": [
    "## Abiotic Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d335ef",
   "metadata": {},
   "source": [
    "### SST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2543b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "oldSSTs = pd.read_csv(\"Data/SSTs\")\n",
    "#For each year I want a list with the SSTs for the months in ascending order. \n",
    "SSTsList = []\n",
    "for year in oldSSTs[\"Year\"].unique():\n",
    "    SSTsList.append([year] + oldSSTs[oldSSTs[\"Year\"] == year][\"SST\"].tolist())\n",
    "\n",
    "SSTs = pd.DataFrame(SSTsList, columns=[\"Year\", \"SST Jan.\", \"SST Feb.\", \"SST Mar.\", \"SST Apr.\", \"SST May\", \"SST Jun.\", \"SST Jul.\", \"SST Aug\", \"SST Sep.\", \"SST Oct.\", \"SST Nov.\", \"SST Dec.\"])\n",
    "SSTs = SSTs[:28]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cd8ca1",
   "metadata": {},
   "source": [
    "### Sea Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10fb983d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Read the Annual Mean Sea Levels\n",
    "        #there are data of four ports\n",
    "#Take the years between start_date and end_date\n",
    "#Take the positive values (non existent data entered as -99999\n",
    "#drop the columns 2 and 3 to keep year and sea level measurement\n",
    "#rename the column that contains Sea Level measurement\n",
    "\n",
    "\n",
    "file_path=\"Data/SeaLevel_Point_Atkinson193.txt\"\n",
    "df=pd.read_csv(file_path, header=None,delimiter=';')\n",
    "df= df[(df[0] >= start_date) & (df[0] <= end_date) & (df[1]>=0)].drop(columns=[2,3])\n",
    "seaLevelPointA_clean =df.rename(columns={0: 'Year', 1: 'Sea Level Point Atkinson'})\n",
    "#missing year 1997\n",
    "\n",
    "file_path=\"Data/SeaLevel_Port_Angeles2127.txt\"\n",
    "df =pd.read_csv(file_path, header=None,delimiter=';')\n",
    "df= df[(df[0] >= start_date) & (df[0] <= end_date) & (df[1]>=0)].drop(columns=[2,3])\n",
    "seaLevelPortA_clean= df.rename(columns={0: 'Year', 1: 'Sea Level Port Angeles'})\n",
    "#no missing year\n",
    "\n",
    "file_path=\"Data/SeaLevel_Campbell_River1323.txt\"\n",
    "df=pd.read_csv(file_path, header=None,delimiter=';')\n",
    "df= df[(df[0] >= start_date) & (df[0] <= end_date) & (df[1]>=0)].drop(columns=[2,3])\n",
    "seaLevelCampbellR_clean= df.rename(columns={0: 'Year', 1: 'Sea Level Campbell River'})\n",
    "#missing year 1995 & 1996\n",
    "\n",
    "file_path=\"Data/SeaLevel_Cherry_annualMean.txt\"\n",
    "df= pd.read_csv(file_path, header=None,delimiter=';')\n",
    "df= df[(df[0] >= start_date) & (df[0] <= end_date) & (df[1]>=0)].drop(columns=[2,3])\n",
    "seaLevelCherryP_clean= df.rename(columns={0: 'Year', 1: 'Sea Level Cherry'})\n",
    "#missing year 1994\n",
    "\n",
    "\n",
    "#Merge the sea Levels\n",
    "#Calculate their average \n",
    "    #missing data is ignored \n",
    "        #note: if a year is missing it's only missing at most one of the ports\n",
    "\n",
    "merged_df= pd.merge(seaLevelPortA_clean, seaLevelPointA_clean, on='Year', how='outer')\n",
    "merged_df = pd.merge(merged_df, seaLevelCampbellR_clean, on='Year', how='outer')\n",
    "seaLevels = pd.merge(merged_df, seaLevelCherryP_clean, on='Year', how='outer')\n",
    "\n",
    "#Average sea levels of four ports\n",
    "seaLevels['av_SeaLevels']= seaLevels.iloc[:,1:].mean(axis=1, skipna=True)\n",
    "#has few NaN values, fill them with the average\n",
    "seaLevels.iloc[7,2] = seaLevels.iloc[7,5]\n",
    "seaLevels.iloc[5,3] = seaLevels.iloc[5,5] \n",
    "seaLevels.iloc[6,3] = seaLevels.iloc[6,5] \n",
    "seaLevels.iloc[4,4] = seaLevels.iloc[4,5] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e5781c",
   "metadata": {},
   "source": [
    "### Salinity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a5be1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Salinity_file = pd.read_excel('Data/Entrance_island_salinity.xlsx') \n",
    "\n",
    "#Sea salinity is provided monthly, we will calculate yearly average. Also, nonexistent data is marked by 999.99\n",
    "\n",
    "df_salinity = Salinity_file.replace(999.99,np.nan)\n",
    "df_salinity_yr = df_salinity['YEAR']\n",
    "df_salinity_val = df_salinity.drop(['YEAR'],axis=1).mean(axis = 1)\n",
    "df_sea_salinity = pd.DataFrame({'Year':df_salinity_yr,'Avg Sea Salinity':df_salinity_val})\n",
    "df_sea_salinity = df_sea_salinity.loc[(df_sea_salinity['Year'] >= start_date) & (df_sea_salinity['Year'] <= end_date)]\n",
    "df_sea_salinity.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4290a1d",
   "metadata": {},
   "source": [
    "### NPGO climate index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caad57e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a climatological index\n",
    "NPGO_monthly = pd.read_csv('Data/NPGOMonthlyAvg copy.txt')\n",
    "\n",
    "# get the yearly NPGO averages and clean it up\n",
    "NPGO_yearly = NPGO_monthly.groupby('Year').mean()\n",
    "NPGO_yearly = NPGO_yearly.drop(columns=['Month']).reset_index()\n",
    "NPGO_yearly = NPGO_yearly.loc[(NPGO_yearly['Year']>=start_date) & (NPGO_yearly['Year']<= end_date)]\n",
    "NPGO_yearly['Year'] = NPGO_yearly['Year'].astype(int)\n",
    "NPGO=  NPGO_yearly.reset_index().drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81a1ca5",
   "metadata": {},
   "source": [
    "### PDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ffc2b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read Pacific decadal oscillation(PDO) data\n",
    "Pdo_file = pd.read_excel('Data/pdo.xlsx','pdo')  \n",
    "\n",
    "\n",
    "df_pdo = Pdo_file.loc[(Pdo_file['year'] >= start_date) & (Pdo_file['year'] <= end_date)]\n",
    "df_pdo_clean = df_pdo.rename(columns={'year':'Year'})\n",
    "df_pdo_avg = df_pdo_clean.groupby(['Year']).mean()\n",
    "df_pdo_avg = df_pdo_avg.drop(columns = ['date','month'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548a7e90",
   "metadata": {},
   "source": [
    "### Flow Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f8e103b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read Fraser river flow data\n",
    "Flow_rate_file = pd.read_excel('Data/Fraser_flow_rate.xlsx','Flow rate')  \n",
    "\n",
    "df_flow_rate = Flow_rate_file[(Flow_rate_file['Year'] >= start_date) & (Flow_rate_file['Year'] <= end_date) &(Flow_rate_file['PARAM'] == 1)]\n",
    "df_flow_rate = df_flow_rate.drop(columns = ['Â ID','PARAM','MM--DD','SYM','MM--DD.1','SYM.1'])\n",
    "df_flow_rate['Avg flow rate'] = 0.5*(df_flow_rate['MAX']+df_flow_rate['MIN'])\n",
    "df_flow_rate.rename(columns = {'MAX':'Max flow rate','MIN':'Min flow rate'},inplace = True)\n",
    "df_flow_rate.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2ee62f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge Environmental Factors\n",
    "    #Salinity, PDO, Fraser River Flow Rate, NPGO, sea levels, SSTs\n",
    "env_factors = pd.merge(df_sea_salinity,df_pdo_avg,on = 'Year',how = 'outer')\n",
    "env_factors = pd.merge(env_factors, df_flow_rate, on = 'Year', how = 'outer')\n",
    "env_factors = pd.merge(env_factors, NPGO, on = 'Year', how = 'outer')\n",
    "env_factors = pd.merge(env_factors, seaLevels, on = 'Year', how = 'outer')\n",
    "env_factors = pd.merge(env_factors, SSTs, on = 'Year', how = 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97da1b5a",
   "metadata": {},
   "source": [
    "## Biotic Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb48f0a",
   "metadata": {},
   "source": [
    "### Predator Populations \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08a23589",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(4) read Killer Whale data\n",
    "Whale_file = pd.read_excel('Data/killerwhales_1970to2020.xlsx','killerwhales_1970to2020') \n",
    "\n",
    "#(5) read harbour seal population data\n",
    "Seal_file = pd.read_excel('Data/harbourseals_1970to2020.xlsx','harbourseals_1970to2020')  \n",
    "\n",
    "\n",
    "df_whale = Whale_file.loc[(Whale_file['Year'] >= start_date) & (Whale_file['Year'] <= end_date)].drop(columns = ['SRKW','srkwSource','nrkwSource'])\n",
    "df_seal = Seal_file.loc[(Seal_file['year'] >= start_date) & (Seal_file['year'] <= end_date)].drop(columns = ['nonSogNt','bcNt']).rename(columns={'year':'Year'})\n",
    "\n",
    "\n",
    "df_whale.reset_index(drop = True, inplace = True)\n",
    "df_seal.reset_index(drop = True, inplace = True)\n",
    "\n",
    "#Merge df_whale and df_seal\n",
    "df_pred_factors = pd.merge(df_seal,df_whale, on = 'Year',how = 'outer')\n",
    "df_pred_factors.rename(columns = {'sogNt':'Harbour seal','NRKW':'Killer whale'},inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f829f7",
   "metadata": {},
   "source": [
    "### Zooplankton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e6dc00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read ZooPlanktonPerryData file\n",
    "\n",
    "xl_file = pd.ExcelFile('Data/ZooPlanktonPerryData.xlsx')\n",
    "\n",
    "Original_Salmon_Sheets = {sheet_name: xl_file.parse(sheet_name) \n",
    "          for sheet_name in xl_file.sheet_names}\n",
    "\n",
    "# Biomass Definitions #No Data here. In case info on column names is needed\n",
    "selected_sheet = \"2. Biomass data definitions\"  \n",
    "BiomassDefs = Original_Salmon_Sheets[selected_sheet]\n",
    "\n",
    "\n",
    "#Extract the zoo plankton anomalies from the excel sheet\n",
    "anom = Original_Salmon_Sheets['3. Zooplankton anomalies']\n",
    "anom = anom[6:28] #Drops years before <=1995 (All Nan values) and years after >=2018\n",
    "zoos = anom.drop(columns=anom.columns[1:3]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "265e040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain  zooplank biomasses\n",
    "##take the years less than end_date \n",
    "##we drop columns that do not give biomass of zooplanktons\n",
    "selected_sheet = \"1. Zooplankton Biomass data\"  \n",
    "df = Original_Salmon_Sheets[selected_sheet]\n",
    "df= df[df[\"yr\"]<=end_date]\n",
    "plankBiomass_data=df.drop(columns=['key', 'survey', 'region', 'event', 'net', 'station', 'lon', 'lat', 'mon', 'day', 'time',  'twilight', 'net.type', 'diam.m', 'mesh.um', 'startz.m', 'endz.m','botz.m',\n",
    "       'volfilt.m3'])\n",
    "\n",
    "# Create a new DataFrame for averaged values\n",
    "#Group by 'yr' and calculate the annual mean of Zooplanktons\n",
    "#drop duplicates\n",
    "#rename the columns for convenienve\n",
    "#reset_index\n",
    "\n",
    "plankBiomassMean = pd.DataFrame()\n",
    "for column in plankBiomass_data.columns:\n",
    "    if column in ['yr', 'region']:\n",
    "        plankBiomassMean[column] = plankBiomass_data[column]\n",
    "    else:\n",
    "        name = 'Av_' + column\n",
    "        plankBiomassMean[name] = plankBiomass_data.groupby('yr')[column].transform('mean')\n",
    "\n",
    "plankBiomassMean.drop_duplicates(subset=['yr'], inplace=True)\n",
    "plankBiomassMean.rename(columns={'yr': 'Year', 'Av_Total.Biomass': 'Av_totalBiomass'},  inplace=True)\n",
    "plankBiomassMean.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a2a36b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plankton_anm_mass = pd.merge(zoos, plankBiomassMean, on = 'Year',how = 'outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db9dd61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "biotic_factors = pd.merge(df_pred_factors, plankton_anm_mass, on = 'Year', how = 'outer')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e4cfff",
   "metadata": {},
   "source": [
    "## Anthropogenic Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8c0d05",
   "metadata": {},
   "source": [
    "### Catches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4fcd106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the relevant data\n",
    "Catches = pd.read_excel(\"Data/Copy of recreational reported fishery catch, 1953-2012.xlsx\")\n",
    "\n",
    "#Next is Catches\n",
    "ModernCatches = Catches[Catches[\"YEAR\"] >= start_date]\n",
    "CohoCatches = ModernCatches[ModernCatches[\"SPECIES_DESC\"] == \"COHO SALMON\"]\n",
    "ChinookCatches = ModernCatches[ModernCatches[\"SPECIES_DESC\"] == \"CHINOOK SALMON\"]\n",
    "cohoAnnualCatchList = []\n",
    "chinookAnnualCatchList = []\n",
    "yearList = []\n",
    "for year in CohoCatches[\"YEAR\"].unique():\n",
    "    cohoAnnualCatchList.append(CohoCatches[CohoCatches[\"YEAR\"] == year][\"PIECES\"].sum())\n",
    "    chinookAnnualCatchList.append(ChinookCatches[ChinookCatches[\"YEAR\"] == year][\"PIECES\"].sum())\n",
    "    yearList.append(year)\n",
    "Catches = pd.DataFrame({\"Coho Catch\": cohoAnnualCatchList, \"Chinook Catch\": chinookAnnualCatchList, \"Year\": yearList})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f978d8a6",
   "metadata": {},
   "source": [
    "### BC and WA regional populations and population trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9aedb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the population data for BC Regional Districts\n",
    "BC_pop = pd.read_csv('Data/BCPop1990.csv')#, index_col=0) # only need index_col=0 to make years the indices\n",
    "BC_pop = BC_pop.rename(columns={'Unnamed: 0':'Year'}).drop_duplicates() # there's a duplicate row at 2011!\n",
    "\n",
    "# pre-2001 BC population numbers are only for every five years\n",
    "# make an empty table for these years, to be interpolated\n",
    "missing_yrs = np.setdiff1d([i for i in range(1990,2001)],[1991,1996])\n",
    "missing_yrs_vals = np.empty((29,9,))*np.nan\n",
    "missing_data = np.vstack((missing_yrs,missing_yrs_vals)).transpose()\n",
    "BC_missing = pd.DataFrame(missing_data)\n",
    "BC_missing.columns = BC_pop.columns\n",
    "BC_pop = pd.concat([BC_missing,BC_pop]).sort_values(by='Year')\n",
    "\n",
    "# fill in the missing years with some linearly interpolated values where possible\n",
    "# and where this doesn't fill in a value, just repeat the closest value\n",
    "BC_pop = BC_pop.interpolate().bfill() \n",
    "# fix the indices obtained from concatenating\n",
    "BC_pop = BC_pop.reset_index().drop(columns=['index'])\n",
    "\n",
    "\n",
    "# import the population data for BC Regional Districts and WA Counties\n",
    "WA_pop = pd.read_csv('Data/WAPop1990.csv')# index_col=0)\n",
    "WA_pop = WA_pop.rename(columns={'Unnamed: 0':'Year'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42363110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BC Group 1 average\n",
    "BC_1 = BC_pop.iloc[:,[3,7,9,10,12,14,18,20,25,28,29]]\n",
    "BC_1_avg = pd.DataFrame(BC_1.mean(axis=1), columns=['BC Gp1 Avg'])\n",
    "# BC Group 2 average\n",
    "BC_2 = BC_pop.iloc[:,[2,5,15,17,19]]\n",
    "BC_2_avg = pd.DataFrame(BC_2.mean(axis=1), columns=['BC Gp2 Avg'])\n",
    "# BC Group 3 average\n",
    "BC_3 = BC_pop.iloc[:,[6,8,11,13,16,22]]\n",
    "BC_3_avg = pd.DataFrame(BC_3.mean(axis=1),columns=['BC Gp3 Avg'])\n",
    "# BC Group 4 average\n",
    "BC_4 = BC_pop.iloc[:,[1,4,24,27]]\n",
    "BC_4_avg = pd.DataFrame(BC_4.mean(axis=1),columns=['BC Gp4 Avg'])\n",
    "# BC Group 5 average\n",
    "BC_5 = BC_pop.iloc[:,[21,23,26]]\n",
    "BC_5_avg = pd.DataFrame(BC_5.mean(axis=1),columns=['BC Gp5 Avg'])\n",
    "\n",
    "# WA Group 1 average\n",
    "WA_1 = WA_pop.iloc[:,[1,3,5,6,8,9,13,16,19,20,21,23,27,29,31,32,34,36,37,39]]\n",
    "WA_1_avg = pd.DataFrame(WA_1.mean(axis=1),columns=['WA Gp1 Avg'])\n",
    "# WA Group 2 average\n",
    "WA_2 = WA_pop.iloc[:,[2,4,18,22,24,25,26,28,30,33,35]]\n",
    "WA_2_avg = pd.DataFrame(WA_2.mean(axis=1),columns=['WA Gp2 Avg'])\n",
    "# WA Group 3 average\n",
    "WA_3 = WA_pop.iloc[:,[11,14,15,17,38]]\n",
    "WA_3_avg = pd.DataFrame(WA_3.mean(axis=1),columns=['WA Gp3 Avg'])\n",
    "# WA Group 4 average\n",
    "WA_4 = WA_pop.iloc[:,[7,10,12]]\n",
    "WA_4_avg = pd.DataFrame(WA_4.mean(axis=1),columns=['WA Gp4 Avg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad20850",
   "metadata": {},
   "source": [
    "##### The grouped population variables\n",
    "\n",
    "It is better to group the regional populations into larger trend groups; these will be the actual population variables to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea6755d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BC_df = pd.concat([  BC_1_avg,\n",
    "                             BC_2_avg,\n",
    "                             BC_3_avg,\n",
    "                             BC_4_avg,\n",
    "                             BC_5_avg ]\n",
    "                  , axis=1)\n",
    "                   \n",
    "WA_df = pd.concat([ WA_1_avg,\n",
    "                             WA_2_avg,\n",
    "                             WA_3_avg ,\n",
    "                             WA_4_avg ]\n",
    "                  , axis=1)\n",
    "\n",
    "populations = pd.concat([WA_pop['Year'] , BC_df, WA_df ]  , axis=1) \n",
    "populations  = populations[:28]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40683f7",
   "metadata": {},
   "source": [
    "### Port of Vancouver tonnage cargo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9dbbcb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data on total tonnage of cargo through Port of Vancouver\n",
    "# years 2008-2023 are given in the Port Metro Vancouver Statistics Overviews at https://www.portvancouver.com/about-us/statistics/\n",
    "# years 1994-1998 are given in The Institutional Position of Seaports An International Comparison by Henrik Stevens, Chapter 7\n",
    "PortofVan = pd.DataFrame({'Year':[1994,1995,1996,1997,1998]+[i for i in range(2008,2023)],  \n",
    "                          'Tonnage':[67600000,71500000,72000000,73500000,71900000]+[114561990,101887824,118378885, \n",
    "                                122499631,123876885,135009878, \n",
    "                                139638157,138082585,135538055, \n",
    "                                142067550,147090934,144225630, \n",
    "                                145450722,146473626,141416326]})\n",
    "\n",
    "missing_yrs_port = pd.DataFrame({'Year':[1990,1991,1992,1993,1999,2000,2001,2002,2003,2004,2005,2006,2007],\n",
    "                               'Tonnage':[np.nan for _ in range(0,13)]})\n",
    "\n",
    "# linearly interpolate the port traffic data\n",
    "Port_of_Van = pd.concat([PortofVan,missing_yrs_port]).sort_values(by=['Year']).interpolate(method='linear', fill_value='extrapolate', limit_direction='both')\n",
    "Port_of_Van = Port_of_Van.reset_index(drop=True)\n",
    "Port_of_Van = Port_of_Van[:28]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7d82aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_effects = pd.merge(Catches, populations,on = 'Year', how = 'outer')\n",
    "human_effects = pd.merge(human_effects, Port_of_Van, on = 'Year', how = 'outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ddf3f",
   "metadata": {},
   "source": [
    "Combine obtained Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a9b250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the relevant data into a single data frame\n",
    "df_merge = pd.merge(env_factors, biotic_factors, on=\"Year\", how=\"outer\")\n",
    "all_factors = pd.merge(df_merge, human_effects, on=\"Year\", how=\"outer\")\n",
    "\n",
    "\n",
    "all_data = pd.merge(Salmon, all_factors, on=\"Year\", how=\"outer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfbdf7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Without year column\n",
    "salmons_features = all_data.drop(columns=['Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d523cc07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cowichan Chinook',\n",
       " 'Harrison Chinook',\n",
       " 'Puntledge Chinook',\n",
       " 'Big Qualicum Coho']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "salmonTypes_List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e07580ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Only one salmon Type\n",
    "type_number=0\n",
    "alldata_CC = all_data.drop(columns= [salmonTypes_List[i] for i in range(4) if i != type_number] ) \n",
    "\n",
    "type_number=1\n",
    "alldata_HC = all_data.drop(columns= [salmonTypes_List[i] for i in range(4) if i != type_number] )\n",
    "\n",
    "type_number = 2\n",
    "alldata_PC = all_data.drop(columns= [salmonTypes_List[i] for i in range(4) if i != type_number] )\n",
    "\n",
    "type_number = 3\n",
    "alldata_BQC = all_data.drop(columns= [salmonTypes_List[i] for i in range(4) if i != type_number] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7061a800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Cowichan Chinook type_number is  0 \n",
      "\n",
      "For Harrison Chinook type_number is  1 \n",
      "\n",
      "For Puntledge Chinook type_number is  2 \n",
      "\n",
      "For Big Qualicum Coho type_number is  3 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(\"For\", salmonTypes_List[i], 'type_number is ', i, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca4013bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No Nan values\n",
    "salmons_features_noNaN = salmons_features.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc551568",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
