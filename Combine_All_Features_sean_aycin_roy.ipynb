{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fc5b03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87be65ff",
   "metadata": {},
   "source": [
    "# Sean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d85478df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the relevant Python modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a15a98af",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Data/SSTs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Import the relevant data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m Salmon \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData/ZooPlanktonPerryData.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m oldSSTs \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mData/SSTs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m Salinity \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData/Entrance_Island_-_Average_Monthly_Sea_Surface_Salinities_1936-2023.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m Catches \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData/Copy of recreational reported fishery catch, 1953-2012.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Data/SSTs'"
     ]
    }
   ],
   "source": [
    "#Import the relevant data\n",
    "Salmon = pd.read_excel(\"Data/ZooPlanktonPerryData.xlsx\", 5)\n",
    "oldSSTs = pd.read_csv(\"Data/SSTs\")\n",
    "Salinity = pd.read_csv(\"Data/Entrance_Island_-_Average_Monthly_Sea_Surface_Salinities_1936-2023.csv\")\n",
    "Catches = pd.read_excel(\"Data/Copy of recreational reported fishery catch, 1953-2012.xlsx\")\n",
    "SeaLevel = pd.read_csv(\"Data/SeaLevel_Cherry_annualMean.txt\", sep=\";\")\n",
    "xl_file = pd.ExcelFile('Data/ZooPlanktonPerryData.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0278c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Salmon = Salmon.rename(columns={\"Ocean Entry Year\":\"Year\"})\n",
    "\n",
    "#For each year I want a list with the SSTs for the months in ascending order. \n",
    "SSTsList = []\n",
    "for year in oldSSTs[\"Year\"].unique():\n",
    "    SSTsList.append([year] + oldSSTs[oldSSTs[\"Year\"] == year][\"SST\"].tolist())\n",
    "\n",
    "SSTs = pd.DataFrame(SSTsList, columns=[\"Year\", \"SST Jan.\", \"SST Feb.\", \"SST Mar.\", \"SST Apr.\", \"SST May\", \"SST Jun.\", \"SST Jul.\", \"SST Aug\", \"SST Sep.\", \"SST Oct.\", \"SST Nov.\", \"SST Dec.\"])\n",
    "\n",
    "#Extract the zoo plankton anomalies from the excel sheet\n",
    "Original_Salmon_Sheets = {sheet_name: xl_file.parse(sheet_name) \n",
    "          for sheet_name in xl_file.sheet_names}\n",
    "\n",
    "anom = Original_Salmon_Sheets['3. Zooplankton anomalies']\n",
    "anom = anom[6:]\n",
    "zoos = anom[anom.columns[3:]]\n",
    "zoos[\"Year\"] = anom[\"Year\"].astype(int)\n",
    "\n",
    "#Grab the relevant years from Salinity\n",
    "Salinity = Salinity[55:] \n",
    "Salinity = Salinity.rename(columns={\"ENTRANCE ISLAND LIGHTSTATION: AVERAGE MONTHLY SEA SURFACE SALINITIES (PSU)\":\"Year\"})\n",
    "Salinity = Salinity['Year'].astype(int)\n",
    "\n",
    "#Next is Catches\n",
    "ModernCatches = Catches[Catches[\"YEAR\"] >= 1990]\n",
    "CohoCatches = ModernCatches[ModernCatches[\"SPECIES_DESC\"] == \"COHO SALMON\"]\n",
    "ChinookCatches = ModernCatches[ModernCatches[\"SPECIES_DESC\"] == \"CHINOOK SALMON\"]\n",
    "cohoAnnualCatchList = []\n",
    "chinookAnnualCatchList = []\n",
    "yearList = []\n",
    "for year in CohoCatches[\"YEAR\"].unique():\n",
    "    cohoAnnualCatchList.append(CohoCatches[CohoCatches[\"YEAR\"] == year][\"PIECES\"].sum())\n",
    "    chinookAnnualCatchList.append(ChinookCatches[ChinookCatches[\"YEAR\"] == year][\"PIECES\"].sum())\n",
    "    yearList.append(year)\n",
    "Catches = pd.DataFrame({\"Coho\": cohoAnnualCatchList, \"Chinook\": chinookAnnualCatchList, \"Year\": yearList})\n",
    "#Sealevel\n",
    "SeaLevel = SeaLevel.drop(['Y', '000'], axis=1)\n",
    "SeaLevel= SeaLevel.rename(columns={' 1985': \"Year\",'  6945': \"Sea Level\"}, errors=\"raise\")\n",
    "SeaLevel = SeaLevel[4:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ce32ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine the relevant data into a single data frame\n",
    "newFrame1 = pd.merge(Salmon, SSTs, on=\"Year\", how=\"outer\")\n",
    "newFrame2 = pd.merge(newFrame1, zoos, on=\"Year\", how=\"outer\")\n",
    "newFrame3 = pd.merge(newFrame2, Salinity, on=\"Year\", how=\"outer\")\n",
    "newFrame4 = pd.merge(newFrame3, Catches, on=\"Year\", how=\"outer\")\n",
    "AllData = pd.merge(newFrame4, SeaLevel, on=\"Year\", how=\"outer\")\n",
    "AllData[[\"Cowichan Chinook\", \"Harrison Chinook\", \"Puntledge Chinook\"]] = AllData[[\"Cowichan Chinook\", \"Harrison Chinook\", \"Puntledge Chinook\"]].fillna(0)\n",
    "#Drop the years we have no viability data for\n",
    "AllData = AllData.drop([28, 29, 30, 31, 32, 33])\n",
    "AllDataNoNull = AllData.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd154575",
   "metadata": {},
   "source": [
    "## Obtained Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f172f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllDataNoNull.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdbbb95",
   "metadata": {},
   "source": [
    "# Roy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcc50648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "start_date = 1996\n",
    "end_date = 2017\n",
    "\n",
    "#(1) read sea salinity data\n",
    "Salinity_file = pd.read_excel('Data/Entrance_island_salinity.xlsx') \n",
    "\n",
    "#(2) read Pacific decadal oscillation(PDO) data\n",
    "Pdo_file = pd.read_excel('Data/pdo.xlsx','pdo')  \n",
    "\n",
    "#(3) read Fraser river flow data\n",
    "Flow_rate_file = pd.read_excel('Data/Fraser_flow_rate.xlsx','Flow rate')  \n",
    "\n",
    "#(4) read Killer Whale data\n",
    "Whale_file = pd.read_excel('Data/killerwhales_1970to2020.xlsx','killerwhales_1970to2020') \n",
    "\n",
    "#(5) read harbour seal population data\n",
    "Seal_file = pd.read_excel('Data/harbourseals_1970to2020.xlsx','harbourseals_1970to2020')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "016ca7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sea salinity is provided monthly, we will calculate yearly average. Also, nonexistent data is marked by 999.99\n",
    "\n",
    "df_salinity = Salinity_file.replace(999.99,np.nan)\n",
    "df_salinity_yr = df_salinity['YEAR']\n",
    "df_salinity_val = df_salinity.drop(['YEAR'],axis=1).mean(axis = 1)\n",
    "df_sea_salinity = pd.DataFrame({'Year':df_salinity_yr,'Avg Sea Salinity':df_salinity_val})\n",
    "df_sea_salinity = df_sea_salinity.loc[(df_sea_salinity['Year'] >= start_date) & (df_sea_salinity['Year'] <= end_date)]\n",
    "df_sea_salinity.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5edb886",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pdo = Pdo_file.loc[(Pdo_file['year'] >= start_date) & (Pdo_file['year'] <= end_date)]\n",
    "df_pdo_clean = df_pdo.rename(columns={'year':'Year'})\n",
    "df_pdo_avg = df_pdo_clean.groupby(['Year']).mean()\n",
    "df_pdo_avg = df_pdo_avg.drop(columns = ['date','month'])\n",
    "\n",
    "\n",
    "df_flow_rate = Flow_rate_file[(Flow_rate_file['Year'] >= start_date) & (Flow_rate_file['Year'] <= end_date) &(Flow_rate_file['PARAM'] == 1)]\n",
    "df_flow_rate = df_flow_rate.drop(columns = ['Â ID','PARAM','MM--DD','SYM','MM--DD.1','SYM.1'])\n",
    "df_flow_rate['Avg flow rate'] = 0.5*(df_flow_rate['MAX']+df_flow_rate['MIN'])\n",
    "df_flow_rate.rename(columns = {'MAX':'Max flow rate','MIN':'Min flow rate'},inplace = True)\n",
    "df_flow_rate.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "392d9437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Year  Avg Sea Salinity       pdo  Max flow rate  Min flow rate  \\\n",
      "0   1996         26.720833  0.683333         8100.0          675.0   \n",
      "1   1997         25.359167  1.316667        11300.0          788.0   \n",
      "2   1998         27.243333 -0.478333         6710.0          600.0   \n",
      "3   1999         26.195833 -1.843333        11000.0          647.0   \n",
      "4   2000         27.405833 -1.125833         8000.0          470.0   \n",
      "5   2001         28.385000 -1.134167         7140.0          478.0   \n",
      "6   2002         27.663333 -0.439167        10600.0          491.0   \n",
      "7   2003         27.967273  0.381667         7300.0          472.0   \n",
      "8   2004         27.438333 -0.224167         6650.0          560.0   \n",
      "9   2005         27.265000 -0.188333         7460.0          805.0   \n",
      "10  2006         28.180833 -0.350833         7190.0          510.0   \n",
      "11  2007         26.918333 -0.702500        10800.0          560.0   \n",
      "12  2008         27.835833 -1.663333        10200.0          648.0   \n",
      "13  2009         27.345000 -1.031667         7490.0          684.0   \n",
      "14  2010         26.639167 -1.060833         5950.0          738.0   \n",
      "15  2011         24.715455 -1.811667         9850.0          700.0   \n",
      "16  2012         25.620833 -1.733333        11700.0          705.0   \n",
      "17  2013         26.575000 -1.164167        10100.0          696.0   \n",
      "18  2014         26.342500  0.552500         9920.0          605.0   \n",
      "19  2015         26.811667  0.915000         7960.0          844.0   \n",
      "20  2016         26.735000  0.670000         6040.0          756.0   \n",
      "21  2017         25.180000 -0.095833         9720.0          627.0   \n",
      "\n",
      "    Avg flow rate  \n",
      "0          4387.5  \n",
      "1          6044.0  \n",
      "2          3655.0  \n",
      "3          5823.5  \n",
      "4          4235.0  \n",
      "5          3809.0  \n",
      "6          5545.5  \n",
      "7          3886.0  \n",
      "8          3605.0  \n",
      "9          4132.5  \n",
      "10         3850.0  \n",
      "11         5680.0  \n",
      "12         5424.0  \n",
      "13         4087.0  \n",
      "14         3344.0  \n",
      "15         5275.0  \n",
      "16         6202.5  \n",
      "17         5398.0  \n",
      "18         5262.5  \n",
      "19         4402.0  \n",
      "20         3398.0  \n",
      "21         5173.5  \n"
     ]
    }
   ],
   "source": [
    "df_env_factors = pd.merge(df_sea_salinity,df_pdo_avg,on = 'Year',how = 'outer')\n",
    "df_env_factors = pd.merge(df_env_factors, df_flow_rate[['Year','Max flow rate','Min flow rate','Avg flow rate']], on = 'Year', how = 'outer')\n",
    "print(df_env_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1ba38d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Avg Sea Salinity</th>\n",
       "      <th>pdo</th>\n",
       "      <th>Max flow rate</th>\n",
       "      <th>Min flow rate</th>\n",
       "      <th>Avg flow rate</th>\n",
       "      <th>Harbour seal</th>\n",
       "      <th>Killer whale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1996</td>\n",
       "      <td>26.720833</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>8100.0</td>\n",
       "      <td>675.0</td>\n",
       "      <td>4387.5</td>\n",
       "      <td>39166</td>\n",
       "      <td>214.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1997</td>\n",
       "      <td>25.359167</td>\n",
       "      <td>1.316667</td>\n",
       "      <td>11300.0</td>\n",
       "      <td>788.0</td>\n",
       "      <td>6044.0</td>\n",
       "      <td>39187</td>\n",
       "      <td>218.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998</td>\n",
       "      <td>27.243333</td>\n",
       "      <td>-0.478333</td>\n",
       "      <td>6710.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>3655.0</td>\n",
       "      <td>39190</td>\n",
       "      <td>216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999</td>\n",
       "      <td>26.195833</td>\n",
       "      <td>-1.843333</td>\n",
       "      <td>11000.0</td>\n",
       "      <td>647.0</td>\n",
       "      <td>5823.5</td>\n",
       "      <td>39190</td>\n",
       "      <td>216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000</td>\n",
       "      <td>27.405833</td>\n",
       "      <td>-1.125833</td>\n",
       "      <td>8000.0</td>\n",
       "      <td>470.0</td>\n",
       "      <td>4235.0</td>\n",
       "      <td>39190</td>\n",
       "      <td>208.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2001</td>\n",
       "      <td>28.385000</td>\n",
       "      <td>-1.134167</td>\n",
       "      <td>7140.0</td>\n",
       "      <td>478.0</td>\n",
       "      <td>3809.0</td>\n",
       "      <td>39190</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2002</td>\n",
       "      <td>27.663333</td>\n",
       "      <td>-0.439167</td>\n",
       "      <td>10600.0</td>\n",
       "      <td>491.0</td>\n",
       "      <td>5545.5</td>\n",
       "      <td>39190</td>\n",
       "      <td>203.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2003</td>\n",
       "      <td>27.967273</td>\n",
       "      <td>0.381667</td>\n",
       "      <td>7300.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>3886.0</td>\n",
       "      <td>39190</td>\n",
       "      <td>204.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2004</td>\n",
       "      <td>27.438333</td>\n",
       "      <td>-0.224167</td>\n",
       "      <td>6650.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>3605.0</td>\n",
       "      <td>39190</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2005</td>\n",
       "      <td>27.265000</td>\n",
       "      <td>-0.188333</td>\n",
       "      <td>7460.0</td>\n",
       "      <td>805.0</td>\n",
       "      <td>4132.5</td>\n",
       "      <td>39190</td>\n",
       "      <td>230.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2006</td>\n",
       "      <td>28.180833</td>\n",
       "      <td>-0.350833</td>\n",
       "      <td>7190.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>3850.0</td>\n",
       "      <td>39190</td>\n",
       "      <td>239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2007</td>\n",
       "      <td>26.918333</td>\n",
       "      <td>-0.702500</td>\n",
       "      <td>10800.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>5680.0</td>\n",
       "      <td>39190</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2008</td>\n",
       "      <td>27.835833</td>\n",
       "      <td>-1.663333</td>\n",
       "      <td>10200.0</td>\n",
       "      <td>648.0</td>\n",
       "      <td>5424.0</td>\n",
       "      <td>39190</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2009</td>\n",
       "      <td>27.345000</td>\n",
       "      <td>-1.031667</td>\n",
       "      <td>7490.0</td>\n",
       "      <td>684.0</td>\n",
       "      <td>4087.0</td>\n",
       "      <td>39190</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2010</td>\n",
       "      <td>26.639167</td>\n",
       "      <td>-1.060833</td>\n",
       "      <td>5950.0</td>\n",
       "      <td>738.0</td>\n",
       "      <td>3344.0</td>\n",
       "      <td>39190</td>\n",
       "      <td>261.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2011</td>\n",
       "      <td>24.715455</td>\n",
       "      <td>-1.811667</td>\n",
       "      <td>9850.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>5275.0</td>\n",
       "      <td>39190</td>\n",
       "      <td>267.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2012</td>\n",
       "      <td>25.620833</td>\n",
       "      <td>-1.733333</td>\n",
       "      <td>11700.0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>6202.5</td>\n",
       "      <td>39190</td>\n",
       "      <td>272.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2013</td>\n",
       "      <td>26.575000</td>\n",
       "      <td>-1.164167</td>\n",
       "      <td>10100.0</td>\n",
       "      <td>696.0</td>\n",
       "      <td>5398.0</td>\n",
       "      <td>39190</td>\n",
       "      <td>275.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2014</td>\n",
       "      <td>26.342500</td>\n",
       "      <td>0.552500</td>\n",
       "      <td>9920.0</td>\n",
       "      <td>605.0</td>\n",
       "      <td>5262.5</td>\n",
       "      <td>39190</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2015</td>\n",
       "      <td>26.811667</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>7960.0</td>\n",
       "      <td>844.0</td>\n",
       "      <td>4402.0</td>\n",
       "      <td>39190</td>\n",
       "      <td>297.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2016</td>\n",
       "      <td>26.735000</td>\n",
       "      <td>0.670000</td>\n",
       "      <td>6040.0</td>\n",
       "      <td>756.0</td>\n",
       "      <td>3398.0</td>\n",
       "      <td>39190</td>\n",
       "      <td>302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2017</td>\n",
       "      <td>25.180000</td>\n",
       "      <td>-0.095833</td>\n",
       "      <td>9720.0</td>\n",
       "      <td>627.0</td>\n",
       "      <td>5173.5</td>\n",
       "      <td>39190</td>\n",
       "      <td>303.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Year  Avg Sea Salinity       pdo  Max flow rate  Min flow rate  \\\n",
       "0   1996         26.720833  0.683333         8100.0          675.0   \n",
       "1   1997         25.359167  1.316667        11300.0          788.0   \n",
       "2   1998         27.243333 -0.478333         6710.0          600.0   \n",
       "3   1999         26.195833 -1.843333        11000.0          647.0   \n",
       "4   2000         27.405833 -1.125833         8000.0          470.0   \n",
       "5   2001         28.385000 -1.134167         7140.0          478.0   \n",
       "6   2002         27.663333 -0.439167        10600.0          491.0   \n",
       "7   2003         27.967273  0.381667         7300.0          472.0   \n",
       "8   2004         27.438333 -0.224167         6650.0          560.0   \n",
       "9   2005         27.265000 -0.188333         7460.0          805.0   \n",
       "10  2006         28.180833 -0.350833         7190.0          510.0   \n",
       "11  2007         26.918333 -0.702500        10800.0          560.0   \n",
       "12  2008         27.835833 -1.663333        10200.0          648.0   \n",
       "13  2009         27.345000 -1.031667         7490.0          684.0   \n",
       "14  2010         26.639167 -1.060833         5950.0          738.0   \n",
       "15  2011         24.715455 -1.811667         9850.0          700.0   \n",
       "16  2012         25.620833 -1.733333        11700.0          705.0   \n",
       "17  2013         26.575000 -1.164167        10100.0          696.0   \n",
       "18  2014         26.342500  0.552500         9920.0          605.0   \n",
       "19  2015         26.811667  0.915000         7960.0          844.0   \n",
       "20  2016         26.735000  0.670000         6040.0          756.0   \n",
       "21  2017         25.180000 -0.095833         9720.0          627.0   \n",
       "\n",
       "    Avg flow rate  Harbour seal  Killer whale  \n",
       "0          4387.5         39166         214.0  \n",
       "1          6044.0         39187         218.0  \n",
       "2          3655.0         39190         216.0  \n",
       "3          5823.5         39190         216.0  \n",
       "4          4235.0         39190         208.0  \n",
       "5          3809.0         39190         200.0  \n",
       "6          5545.5         39190         203.0  \n",
       "7          3886.0         39190         204.0  \n",
       "8          3605.0         39190         220.0  \n",
       "9          4132.5         39190         230.0  \n",
       "10         3850.0         39190         239.0  \n",
       "11         5680.0         39190         240.0  \n",
       "12         5424.0         39190         250.0  \n",
       "13         4087.0         39190         256.0  \n",
       "14         3344.0         39190         261.0  \n",
       "15         5275.0         39190         267.0  \n",
       "16         6202.5         39190         272.0  \n",
       "17         5398.0         39190         275.0  \n",
       "18         5262.5         39190         288.0  \n",
       "19         4402.0         39190         297.0  \n",
       "20         3398.0         39190         302.0  \n",
       "21         5173.5         39190         303.0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_whale = Whale_file.loc[(Whale_file['Year'] >= start_date) & (Whale_file['Year'] <= end_date)].drop(columns = ['SRKW','srkwSource','nrkwSource'])\n",
    "df_seal = Seal_file.loc[(Seal_file['year'] >= start_date) & (Seal_file['year'] <= end_date)].drop(columns = ['nonSogNt','bcNt']).rename(columns={'year':'Year'})\n",
    "\n",
    "\n",
    "df_whale.reset_index(drop = True, inplace = True)\n",
    "df_seal.reset_index(drop = True, inplace = True)\n",
    "\n",
    "df_pred_factors = pd.merge(df_seal,df_whale, on = 'Year',how = 'outer')\n",
    "df_pred_factors.rename(columns = {'sogNt':'Harbour seal','NRKW':'Killer whale'},inplace = True)\n",
    "df_input_variables = pd.merge(df_env_factors,df_pred_factors,on = 'Year', how = 'outer')\n",
    "df_input_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832a39df",
   "metadata": {},
   "source": [
    "# Andrew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15765138",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbf25c2c",
   "metadata": {},
   "source": [
    "# Aycin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e92460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737f7e82",
   "metadata": {},
   "source": [
    "Years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4de76eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Years we concerned with \n",
    "start_year=1990\n",
    "end_year=2017\n",
    "year_range = range(start_year, end_year+1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672868eb",
   "metadata": {},
   "source": [
    " Target Data - Salmon Survivals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589df7e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Read ZooPlanktonPerryData file\n",
    "##obtain SalmonSurvivals_data\n",
    "            #Take years less than or equal to 2017\n",
    "                            #last row (year 2018) has only NaN\n",
    "\n",
    "\n",
    "file_path = \"Data/ZooPlanktonPerryData.xlsx\"\n",
    "selected_sheet = \"5. Salmon marine survivals\"  \n",
    "df = pd.read_excel(file_path, sheet_name=selected_sheet)\n",
    "SalmonSurvivals_data = df[df[\"Ocean Entry Year\"]<=2017]\n",
    "\n",
    "\n",
    "#Rename the columns \n",
    "abbreviation_mapping = {\n",
    "    'Ocean Entry Year': 'year',\n",
    "    'Cowichan Chinook': 'Cow_Ch',\n",
    "    'Harrison Chinook': 'Har_Ch',\n",
    "    'Puntledge Chinook': 'Pun_Ch',\n",
    "    'Big Qualicum Coho': 'BQ_Coho'\n",
    "}\n",
    "\n",
    "salmonSurvivals = SalmonSurvivals_data.rename(columns=abbreviation_mapping)\n",
    "\n",
    "\n",
    "# List of salmon types \n",
    "year_column = 'year'\n",
    "salmonTypes_List =  [col for col in salmonSurvivals.columns if col != year_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505320eb",
   "metadata": {},
   "source": [
    "Data Reads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01567a6",
   "metadata": {},
   "source": [
    "Total Zooplankton Biomasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a27f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read ZooPlanktonPerryData file\n",
    "\n",
    "##obtain  plankBiomass_data   \n",
    "###take the years less than 2017 \n",
    "###we drop columns that do not give biomass of zooplanktons\n",
    "file_path = \"Data/ZooPlanktonPerryData.xlsx\"\n",
    "selected_sheet = \"1. Zooplankton Biomass data\"  \n",
    "df = pd.read_excel(file_path, sheet_name=selected_sheet)\n",
    "df= df[df[\"yr\"]<=2017]\n",
    "plankBiomass_data=df.drop(columns=['key', 'survey', 'region', 'event', 'net', 'station', 'lon', 'lat', 'mon', 'day', 'time',  'twilight', 'net.type', 'diam.m', 'mesh.um', 'startz.m', 'endz.m','botz.m',\n",
    "       'volfilt.m3'])\n",
    "\n",
    "##  BiomassDefs      (if info on column names is needed)\n",
    "selected_sheet = \"2. Biomass data definitions\"  \n",
    "BiomassDefs = pd.read_excel(file_path, sheet_name=selected_sheet)\n",
    "\n",
    "\n",
    "\n",
    "## Create a new DataFrame which contains years and corresponding total biomasses mean\n",
    "#Calculate average Total Biomass for each year inside the common year range\n",
    "#drop duplicates\n",
    "#reset index\n",
    "\n",
    "totalBiomass_av= pd.DataFrame()\n",
    "\n",
    "totalBiomass_av['year']= plankBiomass_data['yr']\n",
    "totalBiomass_av['Av_totalBiomass']= plankBiomass_data.groupby('yr')['Total.Biomass'].transform('mean')\n",
    "totalBiomass_av.drop_duplicates(inplace=True)\n",
    "totalBiomass_av.reset_index(inplace=True, drop=True)\n",
    "\n",
    "\n",
    "# Create a new DataFrame for averaged values\n",
    "#Group by 'yr' and calculate the annual mean of Zooplanktons\n",
    "#drop duplicates\n",
    "#rename the columns for convenienve\n",
    "#reset_index\n",
    "\n",
    "plankBiomassMean = pd.DataFrame()\n",
    "\n",
    "for column in plankBiomass_data.columns:\n",
    "    if column in ['yr', 'region']:\n",
    "        plankBiomassMean[column] = plankBiomass_data[column]\n",
    "    else:\n",
    "        name = 'Av_' + column\n",
    "        plankBiomassMean[name] = plankBiomass_data.groupby('yr')[column].transform('mean')\n",
    "\n",
    "\n",
    "plankBiomassMean.drop_duplicates(subset=['yr'], inplace=True)\n",
    "plankBiomassMean.rename(columns={'yr': 'year', 'Av_Total.Biomass': 'Av_totalBiomass'},  inplace=True)\n",
    "plankBiomassMean.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7616dfc0",
   "metadata": {},
   "source": [
    "### Obtained Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebb5e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(plankBiomassMean.head(3))\n",
    "display(totalBiomass_av.head(3))\n",
    "#Note: data of totalBiomass_av appears in plankBiomassMean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa86d13",
   "metadata": {},
   "source": [
    "Sea Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read the Annual Mean Sea Levels\n",
    "        #there are data of four ports\n",
    "#Take the years between start_year and end_year\n",
    "#Take the positive values (non existent data entered as -99999\n",
    "#drop the columns 2 and 3 to keep year and sea level measurement\n",
    "#rename the column that contains Sea Level measurement\n",
    "\n",
    "\n",
    "file_path=\"Data/SeaLevel_Point_Atkinson193.txt\"\n",
    "df=pd.read_csv(file_path, header=None,delimiter=';')\n",
    "df= df[(df[0] >= start_year) & (df[0] <= end_year) & (df[1]>=0)].drop(columns=[2,3])\n",
    "seaLevelPointA_clean =df.rename(columns={0: 'year', 1: 'SeaLevel_Point_Atkinson'})\n",
    "#missing year 1997\n",
    "\n",
    "file_path=\"Data/SeaLevel_Port_Angeles2127.txt\"\n",
    "df =pd.read_csv(file_path, header=None,delimiter=';')\n",
    "df= df[(df[0] >= start_year) & (df[0] <= end_year) & (df[1]>=0)].drop(columns=[2,3])\n",
    "seaLevelPortA_clean= df.rename(columns={0: 'year', 1: 'SeaLevel_Port_Angeles'})\n",
    "#no missing year\n",
    "\n",
    "file_path=\"Data/SeaLevel_Campbell_River1323.txt\"\n",
    "df=pd.read_csv(file_path, header=None,delimiter=';')\n",
    "df= df[(df[0] >= start_year) & (df[0] <= end_year) & (df[1]>=0)].drop(columns=[2,3])\n",
    "seaLevelCampbellR_clean= df.rename(columns={0: 'year', 1: 'SeaLevel_Campbell_River'})\n",
    "#missing year 1995 & 1996\n",
    "\n",
    "file_path=\"Data/SeaLevel_Cherry_annualMean.txt\"\n",
    "df= pd.read_csv(file_path, header=None,delimiter=';')\n",
    "df= df[(df[0] >= start_year) & (df[0] <= end_year) & (df[1]>=0)].drop(columns=[2,3])\n",
    "seaLevelCherryP_clean= df.rename(columns={0: 'year', 1: 'SeaLevel_Cherry'})\n",
    "#missing year 1994\n",
    "\n",
    "\n",
    "#Merge the sea Levels\n",
    "#Calculate their average \n",
    "    #missing data is ignored \n",
    "        #note: if a year is missing it's only missing at most one of the ports\n",
    "\n",
    "merged_df= pd.merge(seaLevelPortA_clean, seaLevelPointA_clean, on='year', how='outer')\n",
    "merged_df = pd.merge(merged_df, seaLevelCampbellR_clean, on='year', how='outer')\n",
    "seaLevels = pd.merge(merged_df, seaLevelCherryP_clean, on='year', how='outer')\n",
    "\n",
    "seaLevels['av_SeaLevels']= seaLevels.iloc[:,1:].mean(axis=1, skipna=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa84082",
   "metadata": {},
   "source": [
    "### Obtained Data Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d557e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "seaLevels.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f098e0b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a213631",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
